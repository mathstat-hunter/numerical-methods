{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["0-jHkNVM63vM","pM-zfNTX-Y4b","-vNzJHWrHowa","NIuA7V2g3NDa"],"authorship_tag":"ABX9TyPk7nXo7kIUOxOIkS7VSoMT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# nmi | 2024 spring\n","## lecture 06 : systems of equations : symmetric postive-definite\n"],"metadata":{"id":"IgURT4MC1E1V"}},{"cell_type":"markdown","source":["why stop at one? part three.\n","</br>\n"],"metadata":{"id":"NGJug_H6qVd9"}},{"cell_type":"markdown","source":["### 2.6 symmetric positive-definite matrices\n"],"metadata":{"id":"4TCOawUe1ds7"}},{"cell_type":"markdown","source":["if a matrix is symmetric (and positive-definite), some wise guys decided to use half the memory.\n","</br>\n"],"metadata":{"id":"s0fjQq6P1hjD"}},{"cell_type":"markdown","source":["#### 2.6.1 symmetric positive-definite matrices\n"],"metadata":{"id":"Rg2zfjhD20s7"}},{"cell_type":"markdown","source":["<b><font color=grey>def 12</font></b> $n\\times n$ matrix $A$ is <b>symmetric</b> if $A^T = A$. matrix $A$ is <b>positive-definite</b> if $x^TAx > 0$ for all vectors $x\\ne 0$.\n","</br>\n"],"metadata":{"id":"95kRGLin25GO"}},{"cell_type":"markdown","source":["##### ex 26\n"],"metadata":{"id":"0CdaFGPs3fQ1"}},{"cell_type":"markdown","source":["show matrix $A = \\begin{bmatrix} 2 & 2 \\\\ 2 & 5 \\end{bmatrix}$ is symmetric positive-definite.\n","</br>\n"],"metadata":{"id":"hayjvkXq3hk8"}},{"cell_type":"markdown","source":["clearly $A$ is symmetric. to show positive-definite,\n","</br></br>\n","\n","\\begin{align}\n","  x^TAx &=\n","  \\begin{bmatrix} x_1 & x_2 \\end{bmatrix}\n","  \\begin{bmatrix} 2 & 2 \\\\ 2 & 5 \\end{bmatrix}\n","  \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n","  \\\\ \\\\\n","  &= 2x_1^2 + 4x_1x_2 + 5x_2^2 \\\\\n","  &= 2(x_1 + x_2)^2 + 3x_2^2.\n","\\end{align}\n","</br>\n","\n","this expression is always non-negative and cannot be zero unless both $x_2 = 0, x_1+x_2 = 0$, which implies $x = 0$.\n","</br>\n"],"metadata":{"id":"T2YFbEWD3sQz"}},{"cell_type":"markdown","source":["##### ex 27\n"],"metadata":{"id":"I9aWXpOk49In"}},{"cell_type":"markdown","source":["show matrix $A = \\begin{bmatrix} 2 & 4 \\\\ 4 & 5 \\end{bmatrix}$ is not symmetric positive-definite.\n","</br>\n"],"metadata":{"id":"LmQbTw7x49Iy"}},{"cell_type":"markdown","source":["compute $x^TAx$,\n","</br></br>\n","\n","\\begin{align}\n","  x^TAx &=\n","  \\begin{bmatrix} x_1 & x_2 \\end{bmatrix}\n","  \\begin{bmatrix} 2 & 4 \\\\ 4 & 5 \\end{bmatrix}\n","  \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n","  \\\\ \\\\\n","  &= 2x_1^2 + 8x_1x_2 + 5x_2^2 \\\\\n","  &= 2(x_1 + 4x_1x_2)^2 + 5x_2^2 \\\\\n","  &= 2(x_1 + 2x_2)^2 - 8x_2^2 + 5x_2^2 \\\\\n","  &= 2(x_1 + 2x_2)^2 - 3x_2^2 \\\\\n","\\end{align}\n","</br>\n","\n","if $x_1 = -2, x_2 = 1$, then the first term is zero and both terms together are less than zero.\n","</br>\n"],"metadata":{"id":"3ssm2bE249Iy"}},{"cell_type":"markdown","source":["##### <b><font color=grey>property 01</font><b>\n"],"metadata":{"id":"3f3mYvXA6jev"}},{"cell_type":"markdown","source":["if $n\\times n$ matrix $A$ is symmetric, then $A$ is positive-definite iif all its eigenvalues are positive.\n","</br>\n"],"metadata":{"id":"GHjLrATs6k57"}},{"cell_type":"markdown","source":["###### proof\n"],"metadata":{"id":"0-jHkNVM63vM"}},{"cell_type":"markdown","source":["<font color=grey>magic supporting theorem: assume that $A$ is a symmetric $m\\times m$ matrix with real entries. then the eigenvalues are real numbers, and the set of unit eigenvectors of $A$ is an orthonormal set $\\{w_1,\\dots,w_m\\}$ that forms a basis of $\\mathcal{R}^m$.</font>\n","</br></br>\n","\n","<font color=grey>the set of unit eigenvectors is orthonormal and spans $\\mathcal{R}^n$.</font> if $A$ is postive-definite and $Av = \\lambda v$ for nonzero vector $v$, then $0<v^TAv = v^T(\\lambda v) = \\lambda ||v||_2^2$, so $\\lambda > 0$. on the other hand, if all eigenvalues of $A$ are positive, then write any nonzero $x = c_1v_1 + \\dots + c_nv_n$ where the $v_i$ are orthonormal unit vectors and not all $c_i$ are zero. Then $x^TAx = (c_1v_1 + \\dots + c_nv_n)^T(\\lambda_1c_1v_1 + \\dots + \\lambda_nc_nv_n) = \\lambda_1c_1^2 + \\dots + \\lambda_nc_n^2 > 0$, so $A$ is positive-deﬁnite. $\\blacksquare$\n","</br>\n"],"metadata":{"id":"BhuCtU2D65u1"}},{"cell_type":"markdown","source":["##### <b><font color=grey>property 02</font></b>\n"],"metadata":{"id":"qYZIxM4590BW"}},{"cell_type":"markdown","source":["if $A$ is $n\\times n$ symmetric positive-definite and $X$ is $n\\times m$ matrix of [full rank](https://www.mathworks.com/help/matlab/ref/rank.html) with $n\\ge m$, then $X^TAX$ is $m\\times m$ symmetric positive-definite.\n","</br>\n"],"metadata":{"id":"c5GQYiu396qG"}},{"cell_type":"markdown","source":["###### proof\n"],"metadata":{"id":"pM-zfNTX-Y4b"}},{"cell_type":"markdown","source":["The matrix is symmetric since $(X^TAX)^T = X^TAX$. To prove positive-deﬁnite, consider a nonzero $m$-vector $v$. Note that $v^T (X^TAX)v = (Xv)^TA(Xv) \\ge 0$, with equality only if $Xv = 0$, due to the positive-deﬁniteness of $A$. Since $X$ has full rank, its columns are linearly independent, so that $Xv = 0$ implies $v = 0$.\n","</br>\n"],"metadata":{"id":"_MDZQdma-bZ4"}},{"cell_type":"markdown","source":["##### <b><font color=grey>definition 13</font></b>\n"],"metadata":{"id":"HsCE3fQO_sN4"}},{"cell_type":"markdown","source":["<b>principal</b> submatrix of square matrix $A$ is a square submatrix whose diagonal entries are diagonal entries of $A$.\n","</br>\n"],"metadata":{"id":"akT4YAqx_yxK"}},{"cell_type":"markdown","source":["##### <b><font color=grey>property 03</font></b>\n"],"metadata":{"id":"WocuM2WF_8e4"}},{"cell_type":"markdown","source":["any principal submatrix of a symmetric positive-definite matrix is symmetric positive-definite.\n","</br>\n"],"metadata":{"id":"hEu6zKn4AC_Z"}},{"cell_type":"markdown","source":["#### 2.6.2 cholesky factorization\n"],"metadata":{"id":"QlzDoQQGBH4y"}},{"cell_type":"markdown","source":["consider tiny symmetric positive-definite matrix\n","</br></br>\n","\n","$$\\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}.$$\n","</br>\n","\n","by property 03, $a > 0$. determinant $ac-b^2$ is positive bc its product of eigenvalues, all positive by property 01. $A = R^TR$ implies\n","</br></br>\n","\n","\\begin{align}\n","  \\underbrace{\\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}}_{\\text{compare this}}\n","  &=\n","  \\begin{bmatrix} \\sqrt{a} & 0 \\\\ u & v \\end{bmatrix}\n","  \\begin{bmatrix} \\sqrt{a} & u \\\\ 0 & v \\end{bmatrix}\n","  =\n","  \\underbrace{\\begin{bmatrix} a & u\\sqrt{a} \\\\ u\\sqrt{a} & u^2+v^2 \\end{bmatrix}}_{\\text{to this}}\n","  \\\\\n","  \\\\\n","  \\Rightarrow u &= \\frac{b}{\\sqrt{a}} \\\\\n","  v^2 &= c - u^2 \\\\\n","  &= c- \\left(\\frac{b}{\\sqrt{a}}\\right)^2 = c - \\frac{b^2}{a} > 0.\n","\\end{align}\n","</br>\n","\n","ie, $v$ can be defined as real number and cholesky factorization exists\n","</br></br>\n","\n","$$\n","A=\n","\\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}\n","=\n","\\begin{bmatrix}\n","  \\sqrt{a} & 0 \\\\\n","  \\frac{b}{\\sqrt{a}} & \\sqrt{c-\\frac{b^2}{a}}\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","  \\sqrt{a} & \\frac{b}{\\sqrt{a}} \\\\\n","  0 & \\sqrt{c-\\frac{b^2}{a}}\n","\\end{bmatrix}\n","=R^TR.\n","$$\n","</br>\n"],"metadata":{"id":"9mFco1gLBNFb"}},{"cell_type":"markdown","source":["##### <b><font color=grey>theorem 14</font></b> <b>cholesky factorization</b>\n"],"metadata":{"id":"5MIJ-HyEE0h3"}},{"cell_type":"markdown","source":["if $A$ is symmetric positive-definite $n\\times n$ matrix, then there exists an upper triangular $n\\times m$ matrix $R$ such that $A = R^TR$.\n","</br>\n"],"metadata":{"id":"jJLgclOIHZzf"}},{"cell_type":"markdown","source":["###### proof\n"],"metadata":{"id":"-vNzJHWrHowa"}},{"cell_type":"markdown","source":["construct $R$ by induction on size $n$. (case $n=2$ previously.) consider $A$ partitioned as\n","</br></br>\n","\n","$$\n","R=\n","\\begin{bmatrix}\n","a & | && b^T & \\\\\n","-- & - & -- & -- & -- \\\\\n","& | &&& \\\\\n","b & | && C & \\\\\n","& | &&&\n","\\end{bmatrix}\n","$$\n","</br>\n","\n","where $b$ is $(n-1)$-vector and $C$ is $(n-1)\\times(n-1)$ submatrix. use block multiplication to simplify. set $u=\\frac{b}{\\sqrt{a}}$ as in $2\\times 2$ case. set $A_1 = C - uu^T$ and defining invertible matrix\n","</br></br>\n","\n","$$\n","S=\n","\\begin{bmatrix}\n","\\sqrt{a} & | & u^T & \\\\\n","-- & - & ---- & \\\\\n","0 & | && \\\\\n","\\vdots & | & I & \\\\\n","0 & | &&\n","\\end{bmatrix}\n","$$\n","</br>\n","\n","yields\n","</br></br>\n","\n","\\begin{align}\n","  S^T\n","  \\begin{bmatrix}\n","    1 & | & 0 & \\dots & 0 \\\\\n","    -- & - & -- & -- & -- \\\\\n","    0 & | &&& \\\\\n","    \\vdots & | && A_1 & \\\\\n","    0 & | &&&\n","  \\end{bmatrix}\n","  S &=\n","  \\begin{bmatrix}\n","    \\sqrt{a} & | & 0 & \\dots & 0 \\\\\n","    -- & - & -- & -- & -- \\\\\n","    & | &&& \\\\\n","    u & | && I & \\\\\n","    & | &&&\n","  \\end{bmatrix}\n","  \\begin{bmatrix}\n","    1 & | & 0 & \\dots & 0 \\\\\n","    -- & - & -- & -- & -- \\\\\n","    0 & | &&& \\\\\n","    \\vdots & | && A_1 & \\\\\n","    0 & | &&&\n","  \\end{bmatrix}\n","  \\begin{bmatrix}\n","    \\sqrt{a} & | && u^T & \\\\\n","    -- & - & -- & -- & -- \\\\\n","    0 & | &&& \\\\\n","    \\vdots & | && I & \\\\\n","    0 & | &&&\n","  \\end{bmatrix} \\\\\n","  \\\\\n","  &=\n","  \\begin{bmatrix}\n","    a & | && b^T & \\\\\n","    -- & - & -- & -- & -- \\\\\n","    & | &&& \\\\\n","    b & | & uu^T & + & A \\\\\n","    & | &&&\n","  \\end{bmatrix}\n","  = A.\n","\\end{align}\n","</br>\n"],"metadata":{"id":"LEGfMF0LHqRr"}},{"cell_type":"markdown","source":["notice that $A_1$ is symmetric positive-definite. bc\n","</br></br>\n","\n","$$\n","\\begin{bmatrix}\n","  1 & | & 0 & \\vdots & 0 \\\\\n","  -- & - & -- & -- & -- \\\\\n","  0 & | &&& \\\\\n","  \\vdots & | && A_1 & \\\\\n","  0 & | &&&\n","\\end{bmatrix}\n","= (S^T)^{-1}AS^{-1}\n","$$\n","</br>\n","\n","is symmetric positive-definite by property 02. therefore so is $(n-1)\\times(n-1)$ principal submatrix $A_1$ by property 03. by induction, $A_1 = V^TV$ where $V$ is upper triangular. then define upper triangular matrix\n","</br></br>\n","\n","$$\n","R =\n","\\begin{bmatrix}\n","  \\sqrt{a} & | && u^T & \\\\\n","  -- & - & -- & -- & -- \\\\\n","  0 & | &&& \\\\\n","  \\vdots & | && V & \\\\\n","  0 & | &&&\n","\\end{bmatrix}\n","$$\n","</br>\n","\n","and verify that\n","</br></br>\n","\n","$$\n","R^TR =\n","\\begin{bmatrix}\n","  \\sqrt{a} & | & 0 & \\dots & 0 \\\\\n","  -- & - & -- & -- & -- \\\\\n","  & | &&& \\\\\n","  u & | && V^T & \\\\\n","  & | &&&\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","  \\sqrt{a} & | && u^T & \\\\\n","  -- & - & -- & -- & -- \\\\\n","  0 & | &&& \\\\\n","  \\vdots & | && V & \\\\\n","  0 & | &&&\n","\\end{bmatrix}\n","=\n","\\begin{bmatrix}\n","  a & | && b^T & \\\\\n","  -- & - & -- & -- & -- \\\\\n","  & | &&& \\\\\n","  b & | & uu^T & + & V^TV \\\\\n","  & | &&&\n","\\end{bmatrix}\n","= A. \\checkmark\n","$$\n","</br>\n"],"metadata":{"id":"5PUYHhlJosPd"}},{"cell_type":"markdown","source":["the construction of this proof is the algorithm.\n","</br>\n"],"metadata":{"id":"JOKKi2h6qiOq"}},{"cell_type":"markdown","source":["##### algorithm\n"],"metadata":{"id":"xdPQKiLhHtYj"}},{"cell_type":"markdown","source":["<b>cholesky factorization</b>\n","\n","```\n","for k = 1:n\n","  if A[k,k] < 0, stop, end\n","  R[k,k] = √A[k,k]\n","  T(u) = A[k,k+1:n]\\R[k,k]\n","  R[k,k+1:n] = T(u)\n","  A[k+1:n,k+1:n] = A[k+1:n,k+1:n] - uT(u)\n","end\n","```\n","</br>\n","\n","where resulting $R$ satisfies $A=R^TR$.\n","</br>\n"],"metadata":{"id":"Ilymt4bpICG2"}},{"cell_type":"markdown","source":["solving $Ax = b$ for symmetric positive-definite is like $LU$ factorization, $A=R^TR \\Rightarrow R^Tc = b \\Rightarrow Rx = c$.\n","</br>\n"],"metadata":{"id":"pzJLgFmOWbwr"}},{"cell_type":"markdown","source":["also,\n","</br></br>\n","\n","* cholesky without block multiplication [@mathsresource](https://www.youtube.com/watch?v=gFaOa4M12KU).\n","</br>\n"],"metadata":{"id":"kyItPmnNtAmX"}},{"cell_type":"markdown","source":["##### ex 28\n"],"metadata":{"id":"ITdCxmm7SX2z"}},{"cell_type":"markdown","source":["find cholesky factorization of\n","</br></br>\n","\n","$$\n","\\begin{bmatrix}\n","4 & -2 & 2 \\\\\n","-2 & 2 & -4 \\\\\n","2 & -4 & 11\n","\\end{bmatrix}.\n","$$\n","</br>\n"],"metadata":{"id":"AeuNCQBzUeCk"}},{"cell_type":"markdown","source":["the top row of $R$ is $R_{11} = \\sqrt{a_{11}} = 2$, $R_{1,2:3} = \\begin{bmatrix}-2 & 2 \\end{bmatrix}/R_{11} = \\begin{bmatrix}-1 & 1 \\end{bmatrix}$:\n","</br></br>\n","\n","$$\n","R=\n","\\begin{bmatrix}\n","2 & | & -1 && -1 \\\\\n","-- & - & -- & - & -- \\\\\n","& | &&& \\\\\n","& - &&& \\\\\n","& | &&&\n","\\end{bmatrix}.\n","$$\n","</br>\n","\n","subtract outer product $uu^T = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\begin{bmatrix} -1 & 1 \\end{bmatrix}$ from lower principal $2\\times 2$ submatrix $A_{2:3,2:3}$ of $A$ leaves\n","</br></br>\n","\n","$$\n","R=\n","\\begin{bmatrix}\n","& | &&& \\\\\n","-- & - & -- & - & -- \\\\\n","& | & 2 && -4 \\\\\n","& - &&& \\\\\n","& | & -4 && 11\n","\\end{bmatrix}\n","-\n","\\begin{bmatrix}\n","& | &&& \\\\\n","-- & - & -- & - & -- \\\\\n","& | & 1 && -1 \\\\\n","& - &&& \\\\\n","& | & -1 && 1\n","\\end{bmatrix}\n","=\n","\\begin{bmatrix}\n","& | &&& \\\\\n","-- & - & -- & - & -- \\\\\n","& | & 1 && -3 \\\\\n","& - &&& \\\\\n","& | & -3 && 10\n","\\end{bmatrix}.\n","$$\n","</br>\n","\n","repeat the same steps on $2\\times 2$ submatrix to find $R_{22} = 1,R_{23} = \\frac{-3}{1} = -3$\n","</br></br>\n","\n","$$\n","R=\n","\\begin{bmatrix}\n","2 & | & -1 & | & -1 \\\\\n","-- & - & -- & - & -- \\\\\n","& | & 1 & | & -3 \\\\\n","--& - &--& - &-- \\\\\n","& | && | &\n","\\end{bmatrix}.\n","$$\n","</br>\n","\n","the lower $1\\times 1$ principal submatrix of $A$ is $10 - (-3)(-3) = 1$, so $R_{33} = \\sqrt{1}$. the cholesky factor of $A$ is\n","</br></br>\n","\n","$$\n","R=\n","\\begin{bmatrix}\n","2  & -1 & -1 \\\\\n","0 &  1 & -3 \\\\\n","0 & 0 & 1\n","\\end{bmatrix}.\n","$$\n","</br>\n"],"metadata":{"id":"Fe902zzwSZ06"}},{"cell_type":"markdown","source":["#### 2.6.3 conjugate gradient method\n"],"metadata":{"id":"6Gs-5xhFVf6x"}},{"cell_type":"markdown","source":["the [conjugate gradient method](https://www.stat.uchicago.edu/~lekheng/courses/302/classics/hestenes-stiefel.pdf) ([hestens](https://en.wikipedia.org/wiki/Magnus_Hestenes) and [steifel](https://en.wikipedia.org/wiki/Eduard_Stiefel), 1952)\n","(and preconditioners) enhanced solving sparse matrix problems.\n","</br></br>\n","\n","this method tracks down the solution of a positive-definite $n\\times n$ linear system by successively locating and eliminating the $n$ orthogonal components of error, one by one. the complexity of the algorithm is minimized by using the directions established by pairwise orthogonal residual vectors.\n","</br></br>\n","\n","got that? great, the test is tomorrow.\n","</br></br>\n","\n","the conjugate gradient method is based on inner product. ie, euclidean inner product is\n","</br></br>\n","\n","* conjugate symmetric, $\\langle v,w \\rangle = \\overline{\\langle w,v \\rangle}$;\n","</br>\n","* linear, $\\langle \\alpha v + \\beta w,u \\rangle = \\alpha\\langle v,u \\rangle + \\beta\\langle w,u \\rangle$ for scalar $\\alpha,\\beta$;\n","</br>\n","* positive-deﬁnite, $\\langle v,v \\rangle > 0$ if $v \\ne 0$.\n","</br>\n"],"metadata":{"id":"jk5dmCI4Yvsi"}},{"cell_type":"markdown","source":["<b><font color=grey>definition 15</font></b>\n","</br></br>\n","\n","let $A$ be symmetric positive-definite $n\\times n$ matrix. for two $n$-vectors $v,w$, define the $A$-<b>inner product</b>\n","</br></br>\n","\n","$$\\langle v,w \\rangle_A = v^TAw.$$\n","</br>\n","\n","vectors $v,w$ are $A$-<b>conjugate</b> if $\\langle v,w \\rangle_A = 0$.\n","</br>\n"],"metadata":{"id":"_uvcjoMd2GcI"}},{"cell_type":"markdown","source":["note that the new inner product inherits the properties of symmetry, linearity, and positive-definiteness from the matrix $A$. Because $A$ is symmetric, so is the $A$-inner product:\n","</br></br>\n","\n","$$\\langle v,w \\rangle_A = v^TAw = \\langle v^TAw \\rangle^T = w^TAv = \\langle w,v \\rangle_A.$$\n","</br>\n","\n","the $A$-inner product is also linear, and positive-definiteness follows from the fact that if $A$ is positive-definite, then\n","</br></br>\n","\n","$$\\langle v,v \\rangle_A = v^TAv > 0, \\quad v\\ne 0.$$\n","</br>\n","\n","conjugate gradient is a direct method and arrives at solution $x$ of the symmetric positive-definite system $Ax = b$ with a finite loop.\n","</br>\n"],"metadata":{"id":"keTKelCS3JGK"}},{"cell_type":"markdown","source":["##### algorithm\n"],"metadata":{"id":"DeRDUkpZ4eFv"}},{"cell_type":"markdown","source":["<b>conjugate gradient method</b>\n","</br></br>\n","\n","```\n","x0 = initial guess\n","d0 = r0 = b - Ax0\n","for k = 0 : n-1\n","  if r[k] = 0, stop, end\n","\n","  alpha = (T(r[k])·r[k])/(T(d[k])·A·d[k])\n","  x[k+1] = x[k] + alpha*d[k]\n","  r[k+1] = r[k] - alpha*A·d[k]\n","  beta = (T(r[k+1])·r[k+1])/(T(r[k])·r[k])\n","  d[k+1] = r[k+1] + beta*d[k]\n","end\n","```\n","</br>\n"],"metadata":{"id":"uLrDviVD4jrp"}},{"cell_type":"markdown","source":["###### discussion of algorithm\n"],"metadata":{"id":"p4Pp8pDXv_FS"}},{"cell_type":"markdown","source":["conjugate gradient updates three vectors: $x_k$, approximate solution at step $k$; vector $r_k$, residual of $x_k$ where\n","</br></br>\n","\n","\\begin{align}\n","  Ax_{k+1} + r_{k+1} &= A(x_k + \\alpha_kd_k) + r_k - \\alpha_kAd_k \\\\\n","  &= Ax_k + r_k \\\\\n","  &\\Downarrow \\\\\n","  r_k &= b - Ax_k\n","\\end{align}\n","</br>\n","\n","for all $k$; and $d_k$, new search direction of $x_k$. <b>$\\alpha$ is the step-size and $\\beta$ is the correction for the next direction.</b>\n","</br>\n"],"metadata":{"id":"r5GHUipk6aX5"}},{"cell_type":"markdown","source":["this method succeeds bc each residual is orthogonal to previous residuals. in at most $n$ steps, the method will exhaust orthogonal directions, reaching zero residual and correct solution.\n","</br></br>\n","\n","to accomplish orthogonality among residuals relies on choosing pairwise conjguate of search direction $d_k$.\n","</br></br>\n","\n","wrt $\\alpha_k,\\beta_k$, directions $d_k$ is chosen from vector space span of the previous residuals, as seen inductively from the last line of the pseudocode. to ensure that the next residual is orthogonal to all past residuals, $\\alpha_k$ is chosen so that the new residual $r_{k+1}$ is orthogonal to the direction $d_k$:\n","</br></br>\n","\n","\\begin{align}\n","  x_{k+1} &= x_k + \\alpha_kd_k \\\\ \\\\\n","  b-Ax_{k+1} &= b - Ax_k - \\alpha_kAd_k \\\\ \\\\\n","  r_{k+1} &= r_k - \\alpha_kAd_k \\\\ \\\\\n","  0 = d_k^Tr_{k+1} &= d_k^Tr_k - \\alpha_kd_k^TAd_k \\\\ \\\\\n","  \\alpha_k &= \\frac{d_k^Tr_k}{d_k^TAd_k}.\n","\\end{align}\n","</br>\n","\n","$d_{k-1}$ is orthogonal to $r_k$, so\n","</br></br>\n","\n","\\begin{align}\n","  d_k - r_k &= \\beta_{k-1}d_{k-1} \\\\ \\\\\n","  r_k^Td_k - r_k^Tr_k &= 0\n","\\end{align}\n","</br>\n","\n","which justifies rewriting $r_k^Td_k = r_k^Tr_k$ for $\\alpha_k$ of algorithm. then coefficient $\\beta_k$ is chosen for\n","pairwise $A$-conjugacy of the $d_k$:\n","</br></br>\n","\n","\\begin{align}\n","  d_{k+1} &= r_{k+1} + \\beta_kd_k \\\\ \\\\\n","  0 = d_k^TAd_{k+1} &= d_k^TAr_{k+1} + \\beta_kd_k^TAd_k \\\\ \\\\\n","  \\beta_k &= -\\frac{d_k^TAr_{k+1}}{d_k^TAd_k}.\n","\\end{align}\n","</br>\n","\n","the expression for $\\beta_k$ can be rewritten in the simpler form as in the algorithm.\n","</br></br>\n","\n","theorem 16 below verifies that all $r_k$ produced by the conjugate gradient iteration are orthogonal to one another. bc they are $n$-dimensional vectors, at most $n$ of the $r_k$ are pairwise orthogonal, so either $r_n$ or previous $r_k$ must be zero, solving $Ax = b$. therefore, after at most $n$ steps, conjugate gradient arrives at a solution. in theory, the method is a direct, not an iterative, method.\n","</br>\n"],"metadata":{"id":"Gon3ht6_KwFB"}},{"cell_type":"markdown","source":["##### ex 29\n"],"metadata":{"id":"y2TiRNd1sI9I"}},{"cell_type":"markdown","source":["solve system using conjugate gradiant,\n","</br></br>\n","\n","$$\n","\\begin{bmatrix} 2 & 2 \\\\ 2 & 5 \\end{bmatrix}\n","\\begin{bmatrix} u \\\\ v \\end{bmatrix}\n","= \\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix}.\n","$$\n","</br>\n"],"metadata":{"id":"vykBYJi_teVO"}},{"cell_type":"markdown","source":["\\begin{align}\n","  x_0 &= \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n","  r_0 = d_0 = \\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix} \\\\\n","  \\\\\n","  \\alpha &=\n","    \\frac{\n","      \\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix}^T\n","      \\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix}\n","    }{\n","      \\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix}^T\n","      \\begin{bmatrix} 2 & 2 \\\\ 2 & 5 \\end{bmatrix}\n","      \\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix}\n","    }\n","    = \\frac{45}{6\\cdot 18 + 3\\cdot 27} = \\frac{5}{21} \\\\\n","  \\\\\n","  x_1 &=\n","    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n","    + \\frac{5}{21}\\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix}\n","    = \\begin{bmatrix} \\frac{10}{7} \\\\ \\frac{5}{7} \\end{bmatrix} \\\\\n","  r_1 &=\n","    \\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix}\n","    - \\frac{5}{21}\\begin{bmatrix} 18 \\\\ 27 \\end{bmatrix}\n","    = 12\\begin{bmatrix} \\frac{1}{7} \\\\ -\\frac{2}{7} \\end{bmatrix} \\\\\n","  \\\\\n","  \\beta_0 &= \\frac{r_1^Tr_1}{r_0^Tr_0}\n","    = \\frac{144\\cdot\\frac{5}{49}}{36+9} = \\frac{16}{49} \\\\\n","  \\\\\n","  d_1 &= 12\\begin{bmatrix} \\frac{1}{7} \\\\ - \\frac{2}{7} \\end{bmatrix}\n","    + \\frac{16}{49}\\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix}\n","    = \\begin{bmatrix} \\frac{180}{49} \\\\ -\\frac{120}{49} \\end{bmatrix} \\\\\n","  \\\\\n","  \\alpha_1 &=\n","    \\frac{\n","      \\begin{bmatrix} \\frac{12}{7} \\\\ -\\frac{24}{7} \\end{bmatrix}^T\n","      \\begin{bmatrix} \\frac{12}{7} \\\\ -\\frac{24}{7} \\end{bmatrix}\n","    }{\n","      \\begin{bmatrix} \\frac{180}{49} \\\\ -\\frac{120}{49} \\end{bmatrix}^T\n","      \\begin{bmatrix} 2 & 2 \\\\ 2 & 5 \\end{bmatrix}\n","      \\begin{bmatrix} \\frac{180}{49} \\\\ -\\frac{120}{49} \\end{bmatrix}\n","    }\n","    = \\frac{7}{10} \\\\\n","  \\\\\n","  x_2 &= \\begin{bmatrix} \\frac{10}{7} \\\\ \\frac{5}{7} \\end{bmatrix}\n","    + \\frac{7}{10} \\begin{bmatrix} \\frac{180}{49} \\\\ -\\frac{120}{49} \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ -1 \\end{bmatrix} \\\\\n","  \\\\\n","  r_2 &= 12 \\begin{bmatrix} \\frac{1}{7} \\\\ -\\frac{2}{7} \\end{bmatrix}\n","    -\\frac{7}{10} \\begin{bmatrix} 2 & 2 \\\\ 2 & 5 \\end{bmatrix}\n","      \\begin{bmatrix} \\frac{180}{49} \\\\ -\\frac{120}{49} \\end{bmatrix}\n","    = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\\\\n","  \\\\\n","  &\\Downarrow \\\\\n","  \\\\\n","  r_2 &= b - Ax_2 = 0\n","    \\Rightarrow x_2 = \\begin{bmatrix} 4 & -2 \\end{bmatrix}.\n","\\end{align}\n","</br>\n"],"metadata":{"id":"FwaqTSX2t09F"}},{"cell_type":"markdown","source":["##### <b><font color=grey>theorem 16</font></b>\n"],"metadata":{"id":"2_QTYf3xzZDj"}},{"cell_type":"markdown","source":["let $A$ be symmetric positive-definite $n\\times n$ matrix and let $b\\ne 0$ be vector. in conjugate gradient method, assume $r_k\\ne 0$ for $k<n$ (if $r_k=0 \\sim$ equation solved). then for each $1\\le k\\le n$,\n","</br></br>\n","\n","a) the following subspaces of $\\mathbb{R}^n$ are equal:\n","</br>\n","$$\\langle x_1,\\dots,x_k \\rangle = \\langle r_0,\\dots,r_{k-1}\\rangle = \\langle d_0,\\dots,d_{k-1}\\rangle;$$\n","</br>\n","b) residuals $r_k$ are pairwise orthogonal: $r_k^Tr_j = 0, j<k$;\n","</br>\n","c) directions $d_k$ are pairwise $A$-conjugate: $d_k^TAd_j = 0, j<k$.\n","</br></br>\n"],"metadata":{"id":"uU0XpgM30T0L"}},{"cell_type":"markdown","source":["###### proof\n"],"metadata":{"id":"NIuA7V2g3NDa"}},{"cell_type":"markdown","source":["(a) for $k=1$, note $\\langle x_1 \\rangle = \\langle d_0 \\rangle = \\langle r_0 \\rangle$ bc $x_0 = 0$. by def $x_k = x_{k-1} + \\alpha_{k-1}d_{k-1}$. this implies $\\langle x_1,\\dots,x_k\\rangle = \\langle d_0,\\dots,d_{k-1}\\rangle$. similarly, $d_k = r_k + \\beta_{k-1}d_{k-1} \\Rightarrow \\langle r_0,\\dots,r_{k-1}\\rangle = \\langle d_0,\\dots,d_{k-1}\\rangle$\n","</br>\n"],"metadata":{"id":"jzqcygKn3Ooj"}},{"cell_type":"markdown","source":["when $k=0$, self-evident. assume (b),(c) hold for $k$ and prove (b),(c) for $k+1$. multiply def of $r_{k+1}$ by $r_j^T$ on left:\n","</br></br>\n","\n","$$\n","\\underbrace{r_j^T}r_{k+1} = \\underbrace{r_j^T}r_k - \\frac{r_k^Tr_k}{d_k^TAd_k}\\cdot\\underbrace{r_j^T}Ad_k.\n","$$\n","</br>\n","\n","if $j\\le k-1$, then $r_j^Tr_k = 0$ by induction (b). bc $r_j$ can be expressed as combination of $d_0,\\dots,d_j$, term $r_j^TAd_k = 0$ from induction (c) and (b) holds. if $j=k$, then $r_k^Tr_{k+1} = 0$ from previous bc $d_k^TAd_k = r_k^TAd_k + \\beta_{k-1}d_{k-1}^TAd_k = r_k^TAd_k$ using induction (c) proves (b).\n","</br></br>\n","\n","with $r_j^Tr_k = 0$ and previous $r_j^Tr_{k+1}$ with $j=k+1$,\n","</br></br>\n","\n","$$\\frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k} = -\\frac{r_{k+1}^TAd_k}{d_k^TAd_k}.$$\n","</br>\n","\n","multiply def of $d_{k+1}$ from left by $d_j^TA$,\n","</br></br>\n","\n","$$\n","\\underbrace{d_j^T\\cdot A}\\cdot d_{k+1} = \\underbrace{d_j^T\\cdot A}\\cdot r_{k+1} - \\frac{r_{k+1}^T\\cdot A\\cdot d_k}{d_k^T\\cdot A\\cdot d_k}\\cdot\\underbrace{d_j^T\\cdot A}\\cdot d_k.\n","$$\n","</br>\n","\n","if $j = k$, then $d_k^TAd_{k+1} = 0$ using the symmetry of $A$. If $j\\le k - 1$, then $Ad_j = \\frac{r_j - r_j +1}{\\alpha_j}$ (from def\n"," of $r_{k+1}$) is orthogonal to $r_{k+1}$, first term LHS is zero and second term is zero by induction, which proves (c). $\\blacksquare$ i guess.\n","</br>\n"],"metadata":{"id":"A-fdq_gz4wRT"}},{"cell_type":"markdown","source":["##### usw\n"],"metadata":{"id":"jjsld5m0Awnq"}},{"cell_type":"markdown","source":["while that was gnarly, conjugate gradient has advantages: no row operations, no triple loops. however, conjugate gradient has many more operations in general ~ $n^3$ for its $n$ steps vs $\\frac{n^3}{3}$ for gaussian elimination. however, if $A$ is sparse and large enough that $\\frac{n^3}{3}$ is not feasible, conjugate gradient run as iterative may provide a sufficient approximation, hooray.\n","</br></br>\n","\n","but it didnt get play then bc of round-off error for ill-conditioned $A$. <b>preconditioning</b> (which improves the system) compensates for that now.\n","</br>\n"],"metadata":{"id":"Vp9T6INB-Jb-"}},{"cell_type":"markdown","source":["#### 2.6.4 preconditioning\n"],"metadata":{"id":"Bn784E4zAdSZ"}},{"cell_type":"markdown","source":["convergence of iterative methods can be accelerated by use of preconditioning. the idea is to reduce the effective condition number of the problem.\n","</br></br>\n","\n","the preconditioned form of the $n\\times n$ linear system $Ax = b$ is\n","</br></br>\n","\n","$$M^{-1}Ax = M^{-1}b,$$\n","</br>\n","\n","where $M$ is an invertible $n\\times n$ matrix called the <b>preconditioner</b>. an effective preconditioner reduces the condition number of the problem by attempting to invert $A$. conceptually: (1) $M$ should be close to $A$ and (2) simple to invert. (those are not compatible goals, btw.)\n","</br></br>\n","\n","using $M=A$ brings the condition number to one but $A$ is likely difficult to invert if options were sought. $M=I$ does not improve the condition number. so a middle choice like the <b>jacobi preconditioner</b> $M=D=diag(A)$ is fair game.\n","</br>\n"],"metadata":{"id":"A0I4Z_8QBSA7"}},{"cell_type":"markdown","source":["let $z_k = M^{-1}b - M^{-1}Ax_k = M^{-1}r_k$ be preconditioned residual. then\n","</br></br>\n","\n","\\begin{align}\n","  \\alpha_k &= \\frac{(z_k,z_k)M}{(d_k,M^{-1}Ad_k)M} \\\\ \\\\\n","  x_{k+1} &= x_k + \\alpha d_k \\\\ \\\\\n","  z_{k+1} &= z_k - \\alpha M^{-1}Ad_k \\\\ \\\\\n","  \\beta_k &= \\frac{(z_{k+1},z_{k+1})M}{(z_k,z_k)M} \\\\ \\\\\n","  d_{k+1} &= z_{k+1} + \\beta_kd_k, \\\\ \\\\\n","  (z_k,z_k)M &= z_k^TMz_k = z_k^Tr_k \\\\ \\\\\n","  (d_k,M^{-1}Ad_k)M &= d_k^TAd_k \\\\ \\\\\n","  (z_{k+1},z_{k+1})M &= z_{k+1}^TMz_{k+1} = z_{k+1}^Tr_{k+1}.\n","\\end{align}\n","</br>\n"],"metadata":{"id":"YoJxAMfpFMuG"}},{"cell_type":"markdown","source":["##### algorithm\n"],"metadata":{"id":"b3aXaUXeFjln"}},{"cell_type":"markdown","source":["<b>preconditioned conjugate gradient method</b>\n","\n","```\n","x0 = initial guess\n","r0 = b - Ax0\n","d0 = z0 = inv(M)r0\n","for k = 0:n-1\n","  if rk = 0, stop, end\n","\n","  alpha = (T(r[k])·z[k])/(T(d[k])·A·d[k])\n","  x[k+1] = x[k] + alpha·d[k]\n","  r[k+1] = r[k] - alpha·A·d[k]\n","  z[k+1] = inv(M)·r[k+1]\n","  beta = (T(r[k+1])·z[k+1])/(T(r[k])·z[k])\n","  d[k+1] = z[k+1] + beta·d[k]\n","end\n","```\n"],"metadata":{"id":"DJLX7Z5xFmMu"}},{"cell_type":"markdown","source":["to save operations, use back substitution with $M^{-1}$ and not matrix multiplication."],"metadata":{"id":"Zbs0asikHL-Z"}},{"cell_type":"markdown","source":["##### other preconditioners\n"],"metadata":{"id":"fXWmeQMWHbqh"}},{"cell_type":"markdown","source":["<b>symmetric successive over-relaxation (SSOR)</b>\n","</br></br>\n","\n","$$M = (D-\\omega L)D^{-1}(D+\\omega U) = (I+\\omega LD^{-1})(D+\\omega U).$$\n","</br>\n","\n","if $\\omega = 1$, previous is <b>gauss-seidel preconditioner</b>.\n","</br></br>\n","\n","if SSOR used, $z=M^{-1}v$ can be solved with two back substitutions\n","</br></br>\n","\n","\\begin{align}\n","  (I+\\omega LD^{-1})c &= v \\\\\n","  (D+\\omega U)z &= c.\n","\\end{align}\n","</br>\n"],"metadata":{"id":"ydrBQlmJHc2A"}}]}